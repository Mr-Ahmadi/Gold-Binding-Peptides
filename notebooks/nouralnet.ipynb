{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56b57d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-12 17:46:50,278 - INFO - Using device: cpu\n",
      "2025-06-12 17:46:50,283 - INFO - Starting Neural Network model training...\n",
      "2025-06-12 17:46:50,283 - INFO - Loading iFeature_AAC embeddings...\n",
      "2025-06-12 17:46:50,284 - INFO - Loading and extracting features for aac from ../data/embeddings/iFeature_AAC.csv...\n",
      "2025-06-12 17:46:50,295 - INFO - Class distribution: Class 0: 859, Class 1: 861\n",
      "2025-06-12 17:46:50,296 - INFO - iFeature_AAC dimensions: (1720, 20)\n",
      "2025-06-12 17:46:50,297 - INFO - Loading Kidera_encoded embeddings...\n",
      "2025-06-12 17:46:50,297 - INFO - Loading and extracting features for kidera from ../data/embeddings/Kidera_encoded.csv...\n",
      "2025-06-12 17:46:50,306 - INFO - Class distribution: Class 0: 859, Class 1: 861\n",
      "2025-06-12 17:46:50,307 - INFO - Kidera_encoded dimensions: (1720, 10)\n",
      "2025-06-12 17:46:50,307 - INFO - Loading BLOSUM62_encoded embeddings...\n",
      "2025-06-12 17:46:50,307 - INFO - Loading and extracting features for blosum from ../data/embeddings/BLOSUM62_encoded.csv...\n",
      "2025-06-12 17:46:50,314 - INFO - Class distribution: Class 0: 859, Class 1: 861\n",
      "2025-06-12 17:46:50,314 - INFO - BLOSUM62_encoded dimensions: (1720, 20)\n",
      "2025-06-12 17:46:50,315 - INFO - Combined feature dimensions: (1720, 50)\n",
      "2025-06-12 17:46:50,322 - INFO - NN Train dataset size: 1100\n",
      "2025-06-12 17:46:50,322 - INFO - NN Validation dataset size: 276\n",
      "2025-06-12 17:46:50,323 - INFO - Final Test dataset size: 344\n",
      "2025-06-12 17:46:50,323 - INFO - Data scaled using StandardScaler (fitted on combined train+val data)\n",
      "2025-06-12 17:46:50,466 - INFO - Selected 40 features using mutual information from 50 original features\n",
      "2025-06-12 17:46:50,467 - INFO - Positive class weight for NN: 0.9964\n",
      "2025-06-12 17:46:50,477 - INFO - Using CyclicLR learning rate scheduler\n",
      "2025-06-12 17:46:50,478 - INFO - Starting Neural Network training for up to 200 epochs...\n",
      "Epoch 1/200: 100%|██████████| 18/18 [00:00<00:00, 70.95it/s, loss=0.696]\n",
      "2025-06-12 17:46:50,752 - INFO -   NN Epoch 1/200 - Train Loss: 0.6920, Val Loss: 0.6919, LR: 0.000006\n",
      "Epoch 2/200: 100%|██████████| 18/18 [00:00<00:00, 146.14it/s, loss=0.691]\n",
      "2025-06-12 17:46:50,884 - INFO -   NN Epoch 2/200 - Train Loss: 0.6917, Val Loss: 0.6918, LR: 0.000011\n",
      "Epoch 3/200: 100%|██████████| 18/18 [00:00<00:00, 139.55it/s, loss=0.691]\n",
      "2025-06-12 17:46:51,022 - INFO -   NN Epoch 3/200 - Train Loss: 0.6917, Val Loss: 0.6917, LR: 0.000016\n",
      "Epoch 4/200: 100%|██████████| 18/18 [00:00<00:00, 130.50it/s, loss=0.69] \n",
      "2025-06-12 17:46:51,169 - INFO -   NN Epoch 4/200 - Train Loss: 0.6917, Val Loss: 0.6915, LR: 0.000021\n",
      "Epoch 5/200: 100%|██████████| 18/18 [00:00<00:00, 112.15it/s, loss=0.69]\n",
      "2025-06-12 17:46:51,339 - INFO -   NN Epoch 5/200 - Train Loss: 0.6909, Val Loss: 0.6912, LR: 0.000026\n",
      "Epoch 6/200: 100%|██████████| 18/18 [00:00<00:00, 131.36it/s, loss=0.692]\n",
      "2025-06-12 17:46:51,485 - INFO -   NN Epoch 6/200 - Train Loss: 0.6904, Val Loss: 0.6909, LR: 0.000031\n",
      "Epoch 7/200: 100%|██████████| 18/18 [00:00<00:00, 135.95it/s, loss=0.692]\n",
      "2025-06-12 17:46:51,626 - INFO -   NN Epoch 7/200 - Train Loss: 0.6903, Val Loss: 0.6904, LR: 0.000036\n",
      "Epoch 8/200: 100%|██████████| 18/18 [00:00<00:00, 138.11it/s, loss=0.689]\n",
      "2025-06-12 17:46:51,766 - INFO -   NN Epoch 8/200 - Train Loss: 0.6895, Val Loss: 0.6898, LR: 0.000041\n",
      "Epoch 9/200: 100%|██████████| 18/18 [00:00<00:00, 113.37it/s, loss=0.688]\n",
      "2025-06-12 17:46:51,934 - INFO -   NN Epoch 9/200 - Train Loss: 0.6886, Val Loss: 0.6889, LR: 0.000046\n",
      "Epoch 10/200: 100%|██████████| 18/18 [00:00<00:00, 138.27it/s, loss=0.686]\n",
      "2025-06-12 17:46:52,072 - INFO -   NN Epoch 10/200 - Train Loss: 0.6877, Val Loss: 0.6882, LR: 0.000051\n",
      "Epoch 11/200: 100%|██████████| 18/18 [00:00<00:00, 57.72it/s, loss=0.685]\n",
      "2025-06-12 17:46:52,399 - INFO -   NN Epoch 11/200 - Train Loss: 0.6867, Val Loss: 0.6875, LR: 0.000056\n",
      "Epoch 12/200: 100%|██████████| 18/18 [00:00<00:00, 40.59it/s, loss=0.687]\n",
      "2025-06-12 17:46:52,862 - INFO -   NN Epoch 12/200 - Train Loss: 0.6848, Val Loss: 0.6859, LR: 0.000061\n",
      "Epoch 13/200: 100%|██████████| 18/18 [00:00<00:00, 87.22it/s, loss=0.681]\n",
      "2025-06-12 17:46:53,077 - INFO -   NN Epoch 13/200 - Train Loss: 0.6835, Val Loss: 0.6842, LR: 0.000066\n",
      "Epoch 14/200: 100%|██████████| 18/18 [00:00<00:00, 152.48it/s, loss=0.68] \n",
      "2025-06-12 17:46:53,203 - INFO -   NN Epoch 14/200 - Train Loss: 0.6808, Val Loss: 0.6808, LR: 0.000071\n",
      "Epoch 15/200: 100%|██████████| 18/18 [00:00<00:00, 152.16it/s, loss=0.68] \n",
      "2025-06-12 17:46:53,330 - INFO -   NN Epoch 15/200 - Train Loss: 0.6772, Val Loss: 0.6752, LR: 0.000076\n",
      "Epoch 16/200: 100%|██████████| 18/18 [00:00<00:00, 151.33it/s, loss=0.665]\n",
      "2025-06-12 17:46:53,463 - INFO -   NN Epoch 16/200 - Train Loss: 0.6707, Val Loss: 0.6650, LR: 0.000081\n",
      "Epoch 17/200: 100%|██████████| 18/18 [00:00<00:00, 119.38it/s, loss=0.646]\n",
      "2025-06-12 17:46:53,622 - INFO -   NN Epoch 17/200 - Train Loss: 0.6570, Val Loss: 0.6370, LR: 0.000086\n",
      "Epoch 18/200: 100%|██████████| 18/18 [00:00<00:00, 148.46it/s, loss=0.573]\n",
      "2025-06-12 17:46:53,752 - INFO -   NN Epoch 18/200 - Train Loss: 0.6292, Val Loss: 0.6048, LR: 0.000091\n",
      "Epoch 19/200: 100%|██████████| 18/18 [00:00<00:00, 148.47it/s, loss=0.659]\n",
      "2025-06-12 17:46:53,881 - INFO -   NN Epoch 19/200 - Train Loss: 0.5828, Val Loss: 0.5695, LR: 0.000096\n",
      "Epoch 20/200: 100%|██████████| 18/18 [00:00<00:00, 146.81it/s, loss=0.493]\n",
      "2025-06-12 17:46:54,012 - INFO -   NN Epoch 20/200 - Train Loss: 0.5552, Val Loss: 0.5577, LR: 0.000101\n",
      "Epoch 21/200: 100%|██████████| 18/18 [00:00<00:00, 116.27it/s, loss=0.517]\n",
      "2025-06-12 17:46:54,176 - INFO -   NN Epoch 21/200 - Train Loss: 0.5408, Val Loss: 0.5489, LR: 0.000106\n",
      "Epoch 22/200: 100%|██████████| 18/18 [00:00<00:00, 138.26it/s, loss=0.608]\n",
      "2025-06-12 17:46:54,315 - INFO -   NN Epoch 22/200 - Train Loss: 0.5238, Val Loss: 0.5431, LR: 0.000111\n",
      "Epoch 23/200: 100%|██████████| 18/18 [00:00<00:00, 140.16it/s, loss=0.451]\n",
      "2025-06-12 17:46:54,452 - INFO -   NN Epoch 23/200 - Train Loss: 0.5082, Val Loss: 0.5366, LR: 0.000116\n",
      "Epoch 24/200: 100%|██████████| 18/18 [00:00<00:00, 139.03it/s, loss=0.551]\n",
      "2025-06-12 17:46:54,589 - INFO -   NN Epoch 24/200 - Train Loss: 0.5011, Val Loss: 0.5167, LR: 0.000121\n",
      "Epoch 25/200: 100%|██████████| 18/18 [00:00<00:00, 113.22it/s, loss=0.414]\n",
      "2025-06-12 17:46:54,757 - INFO -   NN Epoch 25/200 - Train Loss: 0.4866, Val Loss: 0.4992, LR: 0.000126\n",
      "Epoch 26/200: 100%|██████████| 18/18 [00:00<00:00, 140.67it/s, loss=0.445]\n",
      "2025-06-12 17:46:54,894 - INFO -   NN Epoch 26/200 - Train Loss: 0.4453, Val Loss: 0.4798, LR: 0.000131\n",
      "Epoch 27/200: 100%|██████████| 18/18 [00:00<00:00, 144.37it/s, loss=0.392]\n",
      "2025-06-12 17:46:55,027 - INFO -   NN Epoch 27/200 - Train Loss: 0.4285, Val Loss: 0.4776, LR: 0.000136\n",
      "Epoch 28/200: 100%|██████████| 18/18 [00:00<00:00, 146.55it/s, loss=0.441]\n",
      "2025-06-12 17:46:55,158 - INFO -   NN Epoch 28/200 - Train Loss: 0.4049, Val Loss: 0.4819, LR: 0.000141\n",
      "Epoch 29/200: 100%|██████████| 18/18 [00:00<00:00, 108.04it/s, loss=0.294]\n",
      "2025-06-12 17:46:55,331 - INFO -   NN Epoch 29/200 - Train Loss: 0.4096, Val Loss: 0.4670, LR: 0.000146\n",
      "Epoch 30/200: 100%|██████████| 18/18 [00:00<00:00, 139.36it/s, loss=0.537]\n",
      "2025-06-12 17:46:55,469 - INFO -   NN Epoch 30/200 - Train Loss: 0.4035, Val Loss: 0.4735, LR: 0.000151\n",
      "Epoch 31/200: 100%|██████████| 18/18 [00:00<00:00, 136.04it/s, loss=0.28] \n",
      "2025-06-12 17:46:55,607 - INFO -   NN Epoch 31/200 - Train Loss: 0.3896, Val Loss: 0.4823, LR: 0.000156\n",
      "Epoch 32/200: 100%|██████████| 18/18 [00:00<00:00, 128.32it/s, loss=0.252]\n",
      "2025-06-12 17:46:55,766 - INFO -   NN Epoch 32/200 - Train Loss: 0.3877, Val Loss: 0.4842, LR: 0.000161\n",
      "Epoch 33/200: 100%|██████████| 18/18 [00:00<00:00, 133.02it/s, loss=0.294]\n",
      "2025-06-12 17:46:55,907 - INFO -   NN Epoch 33/200 - Train Loss: 0.3890, Val Loss: 0.4932, LR: 0.000166\n",
      "Epoch 34/200: 100%|██████████| 18/18 [00:00<00:00, 138.54it/s, loss=0.286]\n",
      "2025-06-12 17:46:56,045 - INFO -   NN Epoch 34/200 - Train Loss: 0.3995, Val Loss: 0.4695, LR: 0.000171\n",
      "Epoch 35/200: 100%|██████████| 18/18 [00:00<00:00, 139.81it/s, loss=0.424]\n",
      "2025-06-12 17:46:56,179 - INFO -   NN Epoch 35/200 - Train Loss: 0.3892, Val Loss: 0.4886, LR: 0.000176\n",
      "Epoch 36/200: 100%|██████████| 18/18 [00:00<00:00, 116.98it/s, loss=0.493]\n",
      "2025-06-12 17:46:56,338 - INFO -   NN Epoch 36/200 - Train Loss: 0.3981, Val Loss: 0.4870, LR: 0.000181\n",
      "Epoch 37/200: 100%|██████████| 18/18 [00:00<00:00, 140.93it/s, loss=0.188]\n",
      "2025-06-12 17:46:56,472 - INFO -   NN Epoch 37/200 - Train Loss: 0.3645, Val Loss: 0.4831, LR: 0.000186\n",
      "Epoch 38/200: 100%|██████████| 18/18 [00:00<00:00, 140.66it/s, loss=0.455]\n",
      "2025-06-12 17:46:56,605 - INFO -   NN Epoch 38/200 - Train Loss: 0.3698, Val Loss: 0.4798, LR: 0.000191\n",
      "Epoch 39/200: 100%|██████████| 18/18 [00:00<00:00, 122.66it/s, loss=0.221]\n",
      "2025-06-12 17:46:56,757 - INFO -   NN Epoch 39/200 - Train Loss: 0.3793, Val Loss: 0.4983, LR: 0.000196\n",
      "Epoch 40/200: 100%|██████████| 18/18 [00:00<00:00, 144.82it/s, loss=0.33] \n",
      "2025-06-12 17:46:56,888 - INFO -   NN Epoch 40/200 - Train Loss: 0.3642, Val Loss: 0.4954, LR: 0.000201\n",
      "Epoch 41/200: 100%|██████████| 18/18 [00:00<00:00, 126.33it/s, loss=0.72] \n",
      "2025-06-12 17:46:57,037 - INFO -   NN Epoch 41/200 - Train Loss: 0.3660, Val Loss: 0.4920, LR: 0.000206\n",
      "Epoch 42/200: 100%|██████████| 18/18 [00:00<00:00, 129.50it/s, loss=0.405]\n",
      "2025-06-12 17:46:57,182 - INFO -   NN Epoch 42/200 - Train Loss: 0.3289, Val Loss: 0.5075, LR: 0.000211\n",
      "Epoch 43/200: 100%|██████████| 18/18 [00:00<00:00, 107.13it/s, loss=0.451]\n",
      "2025-06-12 17:46:57,356 - INFO -   NN Epoch 43/200 - Train Loss: 0.3881, Val Loss: 0.4981, LR: 0.000216\n",
      "Epoch 44/200: 100%|██████████| 18/18 [00:00<00:00, 148.20it/s, loss=0.288]\n",
      "2025-06-12 17:46:57,483 - INFO -   NN Epoch 44/200 - Train Loss: 0.3682, Val Loss: 0.5149, LR: 0.000221\n",
      "Epoch 45/200: 100%|██████████| 18/18 [00:00<00:00, 140.07it/s, loss=0.314]\n",
      "2025-06-12 17:46:57,616 - INFO -   NN Epoch 45/200 - Train Loss: 0.3904, Val Loss: 0.5128, LR: 0.000226\n",
      "Epoch 46/200: 100%|██████████| 18/18 [00:00<00:00, 125.53it/s, loss=0.599]\n",
      "2025-06-12 17:46:57,765 - INFO -   NN Epoch 46/200 - Train Loss: 0.3681, Val Loss: 0.5182, LR: 0.000231\n",
      "Epoch 47/200: 100%|██████████| 18/18 [00:00<00:00, 154.74it/s, loss=0.202]\n",
      "2025-06-12 17:46:57,887 - INFO -   NN Epoch 47/200 - Train Loss: 0.3414, Val Loss: 0.5341, LR: 0.000236\n",
      "Epoch 48/200: 100%|██████████| 18/18 [00:00<00:00, 154.32it/s, loss=0.301]\n",
      "2025-06-12 17:46:58,009 - INFO -   NN Epoch 48/200 - Train Loss: 0.3656, Val Loss: 0.5077, LR: 0.000241\n",
      "Epoch 49/200: 100%|██████████| 18/18 [00:00<00:00, 156.66it/s, loss=0.534]\n",
      "2025-06-12 17:46:58,129 - INFO -   NN Epoch 49/200 - Train Loss: 0.3452, Val Loss: 0.5102, LR: 0.000246\n",
      "Epoch 50/200: 100%|██████████| 18/18 [00:00<00:00, 123.72it/s, loss=0.548]\n",
      "2025-06-12 17:46:58,280 - INFO -   NN Epoch 50/200 - Train Loss: 0.3497, Val Loss: 0.5430, LR: 0.000251\n",
      "Epoch 51/200: 100%|██████████| 18/18 [00:00<00:00, 157.83it/s, loss=0.27] \n",
      "2025-06-12 17:46:58,399 - INFO -   NN Epoch 51/200 - Train Loss: 0.3395, Val Loss: 0.5273, LR: 0.000256\n",
      "Epoch 52/200: 100%|██████████| 18/18 [00:00<00:00, 161.25it/s, loss=0.224]\n",
      "2025-06-12 17:46:58,516 - INFO -   NN Epoch 52/200 - Train Loss: 0.3484, Val Loss: 0.5338, LR: 0.000261\n",
      "Epoch 53/200: 100%|██████████| 18/18 [00:00<00:00, 123.42it/s, loss=0.436]\n",
      "2025-06-12 17:46:58,667 - INFO -   NN Epoch 53/200 - Train Loss: 0.3258, Val Loss: 0.5393, LR: 0.000266\n",
      "Epoch 54/200: 100%|██████████| 18/18 [00:00<00:00, 149.57it/s, loss=0.37] \n",
      "2025-06-12 17:46:58,793 - INFO -   NN Epoch 54/200 - Train Loss: 0.3581, Val Loss: 0.5624, LR: 0.000271\n",
      "Epoch 55/200: 100%|██████████| 18/18 [00:00<00:00, 158.67it/s, loss=0.66] \n",
      "2025-06-12 17:46:58,912 - INFO -   NN Epoch 55/200 - Train Loss: 0.3401, Val Loss: 0.5584, LR: 0.000276\n",
      "Epoch 56/200: 100%|██████████| 18/18 [00:00<00:00, 122.80it/s, loss=0.491]\n",
      "2025-06-12 17:46:59,064 - INFO -   NN Epoch 56/200 - Train Loss: 0.3199, Val Loss: 0.5226, LR: 0.000281\n",
      "Epoch 57/200: 100%|██████████| 18/18 [00:00<00:00, 151.29it/s, loss=0.146]\n",
      "2025-06-12 17:46:59,189 - INFO -   NN Epoch 57/200 - Train Loss: 0.3558, Val Loss: 0.5438, LR: 0.000286\n",
      "Epoch 58/200: 100%|██████████| 18/18 [00:00<00:00, 133.76it/s, loss=0.782]\n",
      "2025-06-12 17:46:59,329 - INFO -   NN Epoch 58/200 - Train Loss: 0.3357, Val Loss: 0.5396, LR: 0.000291\n",
      "Epoch 59/200: 100%|██████████| 18/18 [00:00<00:00, 149.01it/s, loss=0.683]\n",
      "2025-06-12 17:46:59,474 - INFO -   NN Epoch 59/200 - Train Loss: 0.3692, Val Loss: 0.5423, LR: 0.000296\n",
      "2025-06-12 17:46:59,475 - INFO -   NN Early stopping triggered at epoch 59. Restoring best model weights.\n",
      "2025-06-12 17:46:59,482 - INFO - Loaded best model weights based on validation loss.\n",
      "2025-06-12 17:46:59,482 - INFO - Training completed. Best epoch: 59\n",
      "2025-06-12 17:46:59,495 - INFO - \n",
      "--- Neural Network (Test) Results ---\n",
      "2025-06-12 17:46:59,496 - INFO - Accuracy: 0.8372\n",
      "2025-06-12 17:46:59,496 - INFO - F1-score: 0.8453\n",
      "2025-06-12 17:46:59,496 - INFO - Precision: 0.8053\n",
      "2025-06-12 17:46:59,496 - INFO - Recall: 0.8895\n",
      "2025-06-12 17:46:59,496 - INFO - ROC AUC: 0.8960\n",
      "2025-06-12 17:46:59,497 - INFO - Confusion Matrix:\n",
      "[[135  37]\n",
      " [ 19 153]]\n",
      "2025-06-12 17:46:59,551 - INFO - ROC curve for Neural Network saved to './images/roc_curve_nn.png'\n",
      "2025-06-12 17:46:59,602 - INFO - Confusion matrix for Neural Network saved to './images/confusion_matrix_nn.png'\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-12 17:46:59,647 - INFO - Neural Network model and artifacts saved to './nn_models'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, roc_auc_score, roc_curve\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CyclicLR\n",
    "import copy\n",
    "import sys\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import joblib\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "\n",
    "# --- Configuration ---\n",
    "# Defines various parameters for data loading, model training, and hyperparameter grids.\n",
    "config = {\n",
    "    'aac_data_path': '../data/embeddings/iFeature_AAC.csv',\n",
    "    'kidera_data_path': '../data/embeddings/Kidera_encoded.csv',\n",
    "    'blosum_data_path': '../data/embeddings/BLOSUM62_encoded.csv',\n",
    "    'intensity_threshold': 207500,\n",
    "    'test_size': 0.2,\n",
    "    'val_size': 0.2, # This is the validation size for the NN, taken from the train_val split\n",
    "    'random_state': 42,\n",
    "    'batch_size': 64,\n",
    "    'epochs': 200,\n",
    "    'patience': 30, # Early stopping patience\n",
    "    'early_stopping_delta': 1e-6, # Minimum change in loss to qualify as an improvement\n",
    "    'learning_rate': 0.001, # Initial learning rate for AdamW\n",
    "    'min_lr': 1e-6, # Minimum LR for CyclicLR\n",
    "    'max_lr': 0.01, # Maximum LR for CyclicLR\n",
    "    'weight_decay': 1e-4, # L2 regularization for AdamW\n",
    "    'hidden_dims_nn': [512, 256, 128, 64, 32], # Hidden layer dimensions for the NN\n",
    "    'dropout_rates_nn': [0.5, 0.4, 0.3, 0.2, 0.1], # Dropout rates for NN layers\n",
    "    'gradient_clip_norm': 1.0, # Gradient clipping to prevent exploding gradients\n",
    "    'noise_magnitude': 0.03, # Magnitude of Gaussian noise for data augmentation\n",
    "    'feature_selection_ratio': 0.8, # Ratio of features to select using SelectKBest\n",
    "    'mixup_alpha': 0.2, # Alpha parameter for Mixup augmentation\n",
    "    'ema_decay': 0.999, # Exponential Moving Average decay for NN weights\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu', # Device for PyTorch (GPU/CPU)\n",
    "    'lr_scheduler_type': 'cyclic', # Type of learning rate scheduler for NN ('cyclic' or 'plateau')\n",
    "}\n",
    "\n",
    "# --- Seed Setting ---\n",
    "def set_seed(seed: int) -> None:\n",
    "    \"\"\"\n",
    "    Sets the random seed for reproducibility across numpy, random, and torch.\n",
    "    This ensures that results are consistent across multiple runs of the code\n",
    "    when using random operations.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Initialize logging and device\n",
    "set_seed(config['random_state'])\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "device = torch.device(config['device'])\n",
    "logging.info(f\"Using device: {device}\")\n",
    "\n",
    "# --- Data Loading and Preprocessing ---\n",
    "class FeatureDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset for handling features and labels, with optional augmentation.\n",
    "    This class provides a standard interface for PyTorch's DataLoader.\n",
    "    \"\"\"\n",
    "    def __init__(self, features, labels, augment=False):\n",
    "        self.features = features # Expects torch.Tensor features\n",
    "        self.labels = labels     # Expects torch.Tensor labels\n",
    "        self.augment = augment\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of samples in the dataset.\"\"\"\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves a sample from the dataset at the given index.\n",
    "        Applies augmentation techniques (Gaussian noise, Mixup) if `self.augment` is True.\n",
    "        \"\"\"\n",
    "        x = self.features[idx]\n",
    "        y = self.labels[idx]\n",
    "        \n",
    "        if self.augment:\n",
    "            # Add Gaussian noise to features for regularization and robustness\n",
    "            noise = torch.randn_like(x) * config['noise_magnitude']\n",
    "            x = x + noise\n",
    "            \n",
    "            # Mixup augmentation: blends the current sample with another random sample\n",
    "            # This helps to regularize the model and improve generalization.\n",
    "            if random.random() > 0.5 and config['mixup_alpha'] > 0:\n",
    "                lam = np.random.beta(config['mixup_alpha'], config['mixup_alpha'])\n",
    "                rand_idx = random.randint(0, len(self.features)-1)\n",
    "                x2 = self.features[rand_idx]\n",
    "                y2 = self.labels[rand_idx]\n",
    "                x = lam * x + (1 - lam) * x2\n",
    "                y = lam * y + (1 - lam) * y2\n",
    "                \n",
    "        return x, y\n",
    "\n",
    "def load_and_extract_features(data_path: str, intensity_threshold: int, embedding_type: str) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Loads a CSV file containing embedding data, extracts relevant features,\n",
    "    and creates a binary target variable based on a specified intensity threshold.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Loading and extracting features for {embedding_type} from {data_path}...\")\n",
    "    try:\n",
    "        df = pd.read_csv(data_path)\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"Error: Data file not found at {data_path}. Please check the path.\")\n",
    "        sys.exit(1) # Exit if file is not found\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading CSV file: {e}\")\n",
    "        sys.exit(1) # Exit on other CSV loading errors\n",
    "\n",
    "    if df.empty:\n",
    "        logging.error(\"Error: Loaded CSV file is empty. No data to process.\")\n",
    "        sys.exit(1) # Exit if DataFrame is empty\n",
    "\n",
    "    # Feature column selection logic based on the type of embedding\n",
    "    if embedding_type == 'aac':\n",
    "        # For AAC, features are typically single-letter amino acid codes.\n",
    "        # This checks for single-character column names that are uppercase letters.\n",
    "        feature_columns = [col for col in df.columns if len(col) == 1 and 'A' <= col <= 'Z' and col not in ['B', 'J', 'O', 'U', 'X', 'Z']]\n",
    "        if not feature_columns:\n",
    "            logging.warning(\"No standard single-letter AAC columns found. Attempting to use all columns except 'Intensity'.\")\n",
    "            feature_columns = [col for col in df.columns if col != 'Intensity']\n",
    "        X = df.loc[:, feature_columns].values.astype(np.float32)\n",
    "    elif embedding_type == 'kidera':\n",
    "        # For Kidera, features are typically named 'KF1' to 'KF10'.\n",
    "        X = df.loc[:, \"KF1\": \"KF10\"].values.astype(np.float32)\n",
    "    elif embedding_type == 'blosum':\n",
    "        # For BLOSUM, features typically start with 'BLOSUM62_'.\n",
    "        blosum_cols = [col for col in df.columns if col.startswith('BLOSUM62_')]\n",
    "        X = df.loc[:, blosum_cols].values.astype(np.float32)\n",
    "    else:\n",
    "        logging.error(f\"Unknown embedding_type: {embedding_type}. Supported types are 'aac', 'kidera', and 'blosum'.\")\n",
    "        sys.exit(1) # Exit on unknown embedding type\n",
    "\n",
    "    if 'Intensity' not in df.columns:\n",
    "        logging.error(\"Error: 'Intensity' column not found in the CSV. Required for target variable creation.\")\n",
    "        sys.exit(1) # Exit if 'Intensity' column is missing\n",
    "\n",
    "    # Create binary target variable: 1 if intensity > threshold, 0 otherwise.\n",
    "    y_class = (df['Intensity'] > intensity_threshold).astype(int).values\n",
    "    class_counts = np.bincount(y_class)\n",
    "    logging.info(f\"Class distribution: Class 0: {class_counts[0]}, Class 1: {class_counts[1]}\")\n",
    "\n",
    "    return X, y_class\n",
    "\n",
    "def select_features(X_train, y_train, X_val, X_test, k_ratio=0.8):\n",
    "    \"\"\"\n",
    "    Performs feature selection using SelectKBest with mutual_info_classif as the scoring function.\n",
    "    It fits the selector only on the training data to prevent data leakage,\n",
    "    then transforms the training, validation, and test sets.\n",
    "    \"\"\"\n",
    "    k = int(X_train.shape[1] * k_ratio) # Determine number of features to select based on ratio\n",
    "    if k == 0: # Ensure at least one feature is selected if k_ratio leads to zero\n",
    "        k = 1\n",
    "    elif k > X_train.shape[1]: # Do not attempt to select more features than available\n",
    "        k = X_train.shape[1]\n",
    "\n",
    "    selector = SelectKBest(mutual_info_classif, k=k)\n",
    "    X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "    X_val_selected = selector.transform(X_val)\n",
    "    X_test_selected = selector.transform(X_test)\n",
    "    logging.info(f\"Selected {k} features using mutual information from {X_train.shape[1]} original features\")\n",
    "    return X_train_selected, X_val_selected, X_test_selected, selector\n",
    "\n",
    "def create_and_scale_splits(X: np.ndarray, y: np.ndarray, test_size: float, val_size: float, random_state: int) -> tuple:\n",
    "    \"\"\"\n",
    "    Splits the combined data into overall training/validation, and test sets.\n",
    "    A single StandardScaler is fitted on the overall training/validation set (X_train_val_raw)\n",
    "    to ensure consistent scaling across all subsets (train, val, test) and prevent data leakage\n",
    "    from the test set into the scaling process.\n",
    "    \"\"\"\n",
    "    # 1. Split the data into an overall training/validation set and a final test set.\n",
    "    sss_test = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
    "    train_val_idx, test_idx = next(sss_test.split(X, y))\n",
    "    X_train_val_raw, X_test_raw = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "\n",
    "    # 2. Fit the StandardScaler ONLY on X_train_val_raw (the data that models will be trained on)\n",
    "    # and then transform all subsets. This prevents data leakage from the test set.\n",
    "    scaler = StandardScaler()\n",
    "    X_train_val_scaled = scaler.fit_transform(X_train_val_raw)\n",
    "    X_test_scaled = scaler.transform(X_test_raw)\n",
    "\n",
    "    # 3. Split the ALREADY SCALED X_train_val_scaled into Neural Network specific train and validation sets.\n",
    "    # This split is also stratified to maintain class proportions.\n",
    "    sss_nn_val = StratifiedShuffleSplit(n_splits=1, test_size=val_size, random_state=random_state)\n",
    "    # y_train_val is used for stratification as it corresponds to X_train_val_scaled\n",
    "    train_nn_idx, val_nn_idx = next(sss_nn_val.split(X_train_val_scaled, y_train_val))\n",
    "    X_train_nn_scaled = X_train_val_scaled[train_nn_idx]\n",
    "    X_val_nn_scaled = X_train_val_scaled[val_nn_idx]\n",
    "    y_train_nn = y_train_val[train_nn_idx]\n",
    "    y_val_nn = y_train_val[val_nn_idx]\n",
    "\n",
    "    logging.info(f\"NN Train dataset size: {len(X_train_nn_scaled)}\")\n",
    "    logging.info(f\"NN Validation dataset size: {len(X_val_nn_scaled)}\")\n",
    "    logging.info(f\"Final Test dataset size: {len(X_test_scaled)}\")\n",
    "    logging.info(\"Data scaled using StandardScaler (fitted on combined train+val data)\")\n",
    "    \n",
    "    # Return all necessary splits and the fitted scaler\n",
    "    return (X_train_nn_scaled, y_train_nn, X_val_nn_scaled, y_val_nn, \n",
    "            X_test_scaled, y_test, scaler, X_train_val_scaled, y_train_val)\n",
    "\n",
    "\n",
    "# --- Neural Network Architecture with Improvements ---\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A Residual Block implementation for the neural network.\n",
    "    It includes two linear layers, Batch Normalization, Dropout for regularization,\n",
    "    and Swish (SiLU) activation function, along with a shortcut connection.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, out_dim, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(in_dim, out_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(out_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.linear2 = nn.Linear(out_dim, out_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(out_dim)\n",
    "        # The shortcut connection ensures that the input can bypass certain layers,\n",
    "        # helping with gradient flow and training of deeper networks.\n",
    "        # If input and output dimensions differ, a linear projection is used for the shortcut.\n",
    "        self.shortcut = nn.Linear(in_dim, out_dim) if in_dim != out_dim else nn.Identity()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity = self.shortcut(x) # Preserve input for the shortcut connection\n",
    "        out = self.linear1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = F.silu(out)  # Swish (SiLU) activation for non-linearity\n",
    "        out = self.dropout(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += identity # Add the shortcut connection output to the main path output\n",
    "        return F.silu(out) # Final activation for the block\n",
    "\n",
    "class AttentionPooling(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements a simple attention mechanism to weigh features within a sample.\n",
    "    Instead of summing, it produces a re-weighted feature vector where important features\n",
    "    are emphasized. This is done by learning attention scores for each feature\n",
    "    and then applying these scores as weights.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_dim):\n",
    "        super().__init__()\n",
    "        # This sequential block generates raw attention scores for each feature.\n",
    "        # It takes an input of shape (batch_size, feature_dim) and outputs\n",
    "        # raw scores of the same shape.\n",
    "        self.attention_score_generator = nn.Sequential(\n",
    "            nn.Linear(feature_dim, feature_dim), # Linear transformation of features\n",
    "            nn.Tanh(),                           # Non-linearity to introduce complexity\n",
    "            nn.Linear(feature_dim, feature_dim)  # Another linear layer to output scores per feature\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x has shape (batch_size, feature_dim), e.g., (64, 32) if feature_dim is 32\n",
    "        raw_attention_scores = self.attention_score_generator(x) # Output also (batch_size, feature_dim)\n",
    "        \n",
    "        # Apply softmax across the feature dimension (dim=1) to normalize scores into\n",
    "        # attention weights that sum to 1 for each sample.\n",
    "        attention_weights = F.softmax(raw_attention_scores, dim=1) # (batch_size, feature_dim)\n",
    "        \n",
    "        # Perform element-wise multiplication: re-weights the original features by attention weights.\n",
    "        # This results in an \"attended\" feature vector, highlighting important features.\n",
    "        attended_features = x * attention_weights # (batch_size, feature_dim)\n",
    "        \n",
    "        return attended_features # The output dimension is preserved to match the next layer's input.\n",
    "\n",
    "\n",
    "class ClassifierNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A multi-layer Neural Network Classifier using Residual Blocks and an Attention Pooling layer.\n",
    "    Designed for binary classification. Includes Exponential Moving Average (EMA)\n",
    "    for model weights, which can help stabilize training and improve performance.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, hidden_dims: list[int], dropout_rates: list[float]):\n",
    "        super(ClassifierNN, self).__init__()\n",
    "        layers = []\n",
    "        \n",
    "        # Initial projection layer to map input features to the first hidden dimension.\n",
    "        self.input_proj = nn.Linear(input_dim, hidden_dims[0])\n",
    "        \n",
    "        # Stack Residual Blocks as defined above. Each block processes and transforms features.\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            layers.append(ResidualBlock(hidden_dims[i], hidden_dims[i+1], dropout_rates[i]))\n",
    "        \n",
    "        self.res_blocks = nn.Sequential(*layers) # Group residual blocks into a sequential module\n",
    "        \n",
    "        # The Attention Pooling layer processes the output of the last residual block.\n",
    "        # It helps the model focus on the most relevant features before the final classification.\n",
    "        self.attention = AttentionPooling(hidden_dims[-1])\n",
    "        \n",
    "        # Final output layer for binary classification. It projects the attended features\n",
    "        # to a single output (logit). Sigmoid activation is applied externally (e.g., in BCEWithLogitsLoss).\n",
    "        self.out = nn.Linear(hidden_dims[-1], 1)\n",
    "        \n",
    "        # EMA model is a copy of the main model, but its weights are updated smoothly\n",
    "        # (exponentially decaying average) during training. It's often used for inference\n",
    "        # as it can provide more stable and often better predictions.\n",
    "        self.ema_model = copy.deepcopy(self)\n",
    "        for param in self.ema_model.parameters():\n",
    "            param.requires_grad = False # EMA model parameters should not be updated by gradient descent directly\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the neural network.\n",
    "        Input `x` is the feature tensor.\n",
    "        Returns the raw logits for binary classification.\n",
    "        \"\"\"\n",
    "        x = F.silu(self.input_proj(x)) # Apply initial projection and Swish activation\n",
    "        x = self.res_blocks(x)         # Pass through the sequence of residual blocks\n",
    "        x = self.attention(x)          # Apply attention pooling to the features\n",
    "        return self.out(x)             # Output the final logits\n",
    "    \n",
    "    def update_ema(self, decay=0.999):\n",
    "        \"\"\"\n",
    "        Updates the weights of the EMA model.\n",
    "        The EMA weights are a weighted average of the current model's weights and previous EMA weights.\n",
    "        \"\"\"\n",
    "        with torch.no_grad(): # No gradient calculation needed for EMA update\n",
    "            for param, ema_param in zip(self.parameters(), self.ema_model.parameters()):\n",
    "                ema_param.data = decay * ema_param.data + (1 - decay) * param.data\n",
    "\n",
    "# --- Early Stopping Helper Class ---\n",
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    A utility class for implementing early stopping during model training.\n",
    "    It monitors a specified validation metric (e.g., validation loss) and\n",
    "    stops training if the metric does not improve for a predefined number of epochs (patience).\n",
    "    It also saves the best model weights observed so far.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience: int = 10, delta: float = 0.0):\n",
    "        self.patience = patience\n",
    "        self.delta = delta # Minimum change in the monitored quantity to qualify as an improvement.\n",
    "        self.best_loss = float('inf') # Initialize with a very large number for comparison\n",
    "        self.counter = 0 # Counter for epochs without improvement\n",
    "        self.best_model_wts = None # To store the state dictionary of the best model\n",
    "        self.early_stop = False # Flag to indicate if early stopping should be triggered\n",
    "\n",
    "    def __call__(self, val_loss: float, model: nn.Module) -> bool:\n",
    "        \"\"\"\n",
    "        Call method to update the early stopping status based on the current validation loss.\n",
    "        \"\"\"\n",
    "        if val_loss < self.best_loss - self.delta: # If current loss is an improvement over the best\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model_wts = copy.deepcopy(model.state_dict()) # Save a deep copy of the model's current weights\n",
    "            self.counter = 0 # Reset the patience counter\n",
    "        else: # If loss does not improve (or improvement is less than delta)\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True # Trigger early stopping\n",
    "        return self.early_stop\n",
    "\n",
    "    def load_best_weights(self, model: nn.Module) -> None:\n",
    "        \"\"\"\n",
    "        Loads the best saved model weights back into the provided model.\n",
    "        This is typically called after training completes (either by reaching max epochs or early stopping).\n",
    "        \"\"\"\n",
    "        if self.best_model_wts:\n",
    "            model.load_state_dict(self.best_model_wts)\n",
    "            logging.info(\"Loaded best model weights based on validation loss.\")\n",
    "        else:\n",
    "            logging.warning(\"No best model weights saved, possibly due to training not starting or errors.\")\n",
    "\n",
    "# --- Learning Rate Finder (utility, not used in main execution by default) ---\n",
    "def find_learning_rate(model, train_loader, criterion, optimizer, device, min_lr=1e-7, max_lr=1, num_iters=100):\n",
    "    \"\"\"\n",
    "    Performs a learning rate range test. This utility function helps to identify\n",
    "    a good range for learning rates by incrementally increasing the LR and observing the loss.\n",
    "    It's not used in the main training loop but can be helpful for hyperparameter tuning.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    # Calculate the multiplicative factor to increase LR exponentially over num_iters\n",
    "    lr_mult = (max_lr / min_lr) ** (1/num_iters) \n",
    "    lr = min_lr\n",
    "    optimizer.param_groups[0]['lr'] = lr # Set the initial learning rate\n",
    "    \n",
    "    losses = []\n",
    "    lrs = []\n",
    "    \n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        if i >= num_iters:\n",
    "            break # Stop after the specified number of iterations\n",
    "            \n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad() # Clear gradients from the previous step\n",
    "        outputs = model(inputs).squeeze()\n",
    "        loss = criterion(outputs, targets.float())\n",
    "        \n",
    "        # Backpropagate the loss and update model weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update learning rate for the next iteration\n",
    "        lr *= lr_mult\n",
    "        optimizer.param_groups[0]['lr'] = lr\n",
    "        \n",
    "        # Record loss and learning rate for plotting/analysis\n",
    "        losses.append(loss.item())\n",
    "        lrs.append(lr)\n",
    "        \n",
    "    return lrs, losses\n",
    "\n",
    "# --- Training and Evaluation Functions ---\n",
    "def train_nn_model(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    scheduler: torch.optim.lr_scheduler._LRScheduler,\n",
    "    early_stopper: EarlyStopping,\n",
    "    epochs: int,\n",
    "    device: torch.device,\n",
    "    gradient_clip_norm: float\n",
    ") -> nn.Module:\n",
    "    \"\"\"\n",
    "    Orchestrates the training process for the neural network model.\n",
    "    Includes iterative training, validation, learning rate scheduling,\n",
    "    gradient clipping, and early stopping.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Starting Neural Network training for up to {epochs} epochs...\")\n",
    "    best_epoch = 0 # Tracks the epoch where the best validation loss was achieved\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train() # Set the model to training mode (enables dropout, BatchNorm updates)\n",
    "        current_train_loss = 0.0\n",
    "        # tqdm provides a progress bar for iterables, useful for long training loops.\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs}\")\n",
    "        \n",
    "        for xb, yb in progress_bar:\n",
    "            xb, yb = xb.to(device), yb.to(device) # Move batch data to the specified device (CPU/GPU)\n",
    "            optimizer.zero_grad() # Reset gradients to zero before backpropagation\n",
    "            preds = model(xb).squeeze() # Perform a forward pass, .squeeze() removes singleton dimensions\n",
    "            loss = criterion(preds, yb.float()) # Calculate the loss (e.g., BCEWithLogitsLoss)\n",
    "            loss.backward() # Compute gradients based on the loss\n",
    "            \n",
    "            # Apply gradient clipping to prevent exploding gradients, which can destabilize training.\n",
    "            if gradient_clip_norm > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clip_norm)\n",
    "                \n",
    "            optimizer.step() # Update model parameters using the computed gradients\n",
    "            model.update_ema(config['ema_decay']) # Update the Exponential Moving Average weights\n",
    "            \n",
    "            current_train_loss += loss.item() * xb.size(0) # Accumulate weighted loss for epoch average\n",
    "            progress_bar.set_postfix(loss=loss.item()) # Display current batch loss in the progress bar\n",
    "            \n",
    "        current_train_loss /= len(train_loader.dataset) # Calculate average training loss for the epoch\n",
    "\n",
    "        # Validation phase: evaluate model performance on unseen data\n",
    "        model.eval() # Set the model to evaluation mode (disables dropout, BatchNorm uses running stats)\n",
    "        current_val_loss = 0.0\n",
    "        with torch.no_grad(): # Disable gradient calculation for validation (saves memory and speeds up)\n",
    "            for xb, yb in val_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                preds = model(xb).squeeze()\n",
    "                loss = criterion(preds, yb.float())\n",
    "                current_val_loss += loss.item() * xb.size(0)\n",
    "        current_val_loss /= len(val_loader.dataset) # Calculate average validation loss for the epoch\n",
    "\n",
    "        # Step the learning rate scheduler\n",
    "        if isinstance(scheduler, ReduceLROnPlateau):\n",
    "            scheduler.step(current_val_loss) # ReduceLROnPlateau adjusts LR based on validation loss\n",
    "        else:\n",
    "            scheduler.step() # Other schedulers (like CyclicLR) might step per epoch or iteration\n",
    "\n",
    "        logging.info(f\"  NN Epoch {epoch}/{epochs} - Train Loss: {current_train_loss:.4f}, Val Loss: {current_val_loss:.4f}, LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "        # Check early stopping criteria\n",
    "        if early_stopper(current_val_loss, model):\n",
    "            logging.info(f\"  NN Early stopping triggered at epoch {epoch}. Restoring best model weights.\")\n",
    "            best_epoch = epoch\n",
    "            break # Exit the training loop if early stopping condition is met\n",
    "            \n",
    "        if epoch == epochs: # If training completes all specified epochs without early stopping\n",
    "            best_epoch = epoch\n",
    "\n",
    "    # After the training loop, load the best model weights saved by the early stopper.\n",
    "    early_stopper.load_best_weights(model) \n",
    "    logging.info(f\"Training completed. Best epoch: {best_epoch}\")\n",
    "    return model\n",
    "\n",
    "def evaluate_classifier_metrics(y_true: np.ndarray, y_pred: np.ndarray, y_prob: np.ndarray = None, model_name: str = \"Model\") -> tuple:\n",
    "    \"\"\"\n",
    "    Calculates and logs a comprehensive set of classification performance metrics,\n",
    "    including accuracy, F1-score, precision, recall, ROC AUC, and the confusion matrix.\n",
    "    \"\"\"\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    roc_auc = np.nan # Initialize ROC AUC as NaN; it will be computed if probabilities are available\n",
    "    if y_prob is not None and len(np.unique(y_true)) > 1: # ROC AUC requires probabilities and at least two classes\n",
    "        try:\n",
    "            roc_auc = roc_auc_score(y_true, y_prob)\n",
    "        except ValueError as e:\n",
    "            logging.warning(f\"Error computing ROC AUC for {model_name}: {e}\") # Log a warning if computation fails\n",
    "\n",
    "    logging.info(f\"\\n--- {model_name} Results ---\")\n",
    "    logging.info(f\"Accuracy: {accuracy:.4f}\")\n",
    "    logging.info(f\"F1-score: {f1:.4f}\")\n",
    "    logging.info(f\"Precision: {precision:.4f}\")\n",
    "    logging.info(f\"Recall: {recall:.4f}\")\n",
    "    logging.info(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "    logging.info(f\"Confusion Matrix:\\n{cm}\")\n",
    "\n",
    "    return accuracy, f1, precision, recall, roc_auc, cm\n",
    "\n",
    "def plot_roc_curve(y_true: np.ndarray, y_prob: np.ndarray, model_name: str, save_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Plots the Receiver Operating Characteristic (ROC) curve and saves it to a file.\n",
    "    \"\"\"\n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        logging.warning(f\"Cannot plot ROC curve for {model_name}: Only one class present in y_true.\")\n",
    "        return\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_prob)\n",
    "    roc_auc = roc_auc_score(y_true, y_prob)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'Receiver Operating Characteristic for {model_name}')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(save_path)\n",
    "    logging.info(f\"ROC curve for {model_name} saved to '{save_path}'\")\n",
    "    plt.close() # Close the plot to free memory\n",
    "\n",
    "\n",
    "# --- Main Script Execution ---\n",
    "def main():\n",
    "    logging.info(\"Starting Neural Network model training...\")\n",
    "\n",
    "    # --- Data Loading ---\n",
    "    # Load feature embeddings from different sources. Each embedding type (AAC, Kidera, BLOSUM)\n",
    "    # provides a different representation of the input data, potentially capturing diverse patterns.\n",
    "    logging.info(\"Loading iFeature_AAC embeddings...\")\n",
    "    X_aac, y_class_aac = load_and_extract_features(config['aac_data_path'], config['intensity_threshold'], 'aac')\n",
    "    logging.info(f\"iFeature_AAC dimensions: {X_aac.shape}\")\n",
    "\n",
    "    logging.info(\"Loading Kidera_encoded embeddings...\")\n",
    "    X_kidera, y_class_kidera = load_and_extract_features(config['kidera_data_path'], config['intensity_threshold'], 'kidera')\n",
    "    logging.info(f\"Kidera_encoded dimensions: {X_kidera.shape}\")\n",
    "\n",
    "    logging.info(\"Loading BLOSUM62_encoded embeddings...\")\n",
    "    X_blosum, y_class_blosum = load_and_extract_features(config['blosum_data_path'], config['intensity_threshold'], 'blosum')\n",
    "    logging.info(f\"BLOSUM62_encoded dimensions: {X_blosum.shape}\")\n",
    "\n",
    "    # Verify that the target variables extracted from all embedding files are identical.\n",
    "    # This is crucial for ensuring data alignment across different feature sets.\n",
    "    if not (np.array_equal(y_class_aac, y_class_kidera) and np.array_equal(y_class_aac, y_class_blosum)):\n",
    "        logging.error(\"Target variables from different embeddings are not identical. Check input data alignment.\")\n",
    "        sys.exit(1) # Exit if misalignment is detected\n",
    "    y_class = y_class_aac # Use one of them as the definitive target variable\n",
    "\n",
    "    # Combine all feature sets horizontally (concatenate columns).\n",
    "    # This creates a comprehensive feature vector for each sample.\n",
    "    X_combined = np.hstack((X_aac, X_kidera, X_blosum))\n",
    "    logging.info(f\"Combined feature dimensions: {X_combined.shape}\")\n",
    "\n",
    "    # --- Data Splitting and Scaling ---\n",
    "    # Split the combined data into training, validation, and test sets.\n",
    "    # Scaling is applied based on the training data only to prevent data leakage.\n",
    "    (X_train_nn_scaled, y_train_nn, X_val_nn_scaled, y_val_nn, \n",
    "     X_test_scaled, y_test, scaler_global, _, _) = \\\n",
    "        create_and_scale_splits(X_combined, y_class, config['test_size'], config['val_size'], config['random_state'])\n",
    "\n",
    "    # --- Feature Selection ---\n",
    "    # Apply feature selection using mutual information to reduce dimensionality and noise.\n",
    "    # The selector is fitted on the neural network's training data.\n",
    "    X_train_nn_selected, X_val_nn_selected, X_test_selected, feature_selector = \\\n",
    "        select_features(X_train_nn_scaled, y_train_nn, X_val_nn_scaled, X_test_scaled, \n",
    "                        config['feature_selection_ratio'])\n",
    "    \n",
    "    input_dim_selected = X_train_nn_selected.shape[1] # The effective input dimension for the NN\n",
    "\n",
    "    # --- PyTorch DataLoader Preparation for Neural Network ---\n",
    "    # Convert NumPy arrays to PyTorch tensors and create datasets.\n",
    "    tensor_X_train_nn = torch.tensor(X_train_nn_selected, dtype=torch.float32)\n",
    "    tensor_y_train_nn = torch.tensor(y_train_nn, dtype=torch.float32)\n",
    "    tensor_X_val_nn = torch.tensor(X_val_nn_selected, dtype=torch.float32)\n",
    "    tensor_y_val_nn = torch.tensor(y_val_nn, dtype=torch.float32)\n",
    "    tensor_X_test = torch.tensor(X_test_selected, dtype=torch.float32)\n",
    "    tensor_y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "    # Create PyTorch datasets, applying augmentation to the training data.\n",
    "    train_nn_ds = FeatureDataset(tensor_X_train_nn, tensor_y_train_nn, augment=True)\n",
    "    val_nn_ds = TensorDataset(tensor_X_val_nn, tensor_y_val_nn)\n",
    "    test_nn_ds = TensorDataset(tensor_X_test, tensor_y_test)\n",
    "\n",
    "    # Create DataLoaders for efficient batching and shuffling of data during training.\n",
    "    train_nn_loader = DataLoader(train_nn_ds, batch_size=config['batch_size'], shuffle=True)\n",
    "    val_nn_loader = DataLoader(val_nn_ds, batch_size=config['batch_size'], shuffle=False)\n",
    "    test_nn_loader = DataLoader(test_nn_ds, batch_size=config['batch_size'], shuffle=False)\n",
    "\n",
    "    # --- Neural Network Model Training ---\n",
    "    # Calculate class weights for the BCEWithLogitsLoss. This is important for\n",
    "    # handling imbalanced datasets by giving more importance to the minority class.\n",
    "    count_class_0 = np.sum(y_train_nn == 0)\n",
    "    count_class_1 = np.sum(y_train_nn == 1)\n",
    "    pos_weight = count_class_0 / count_class_1 if count_class_1 > 0 else 1.0\n",
    "    logging.info(f\"Positive class weight for NN: {pos_weight:.4f}\")\n",
    "\n",
    "    # Initialize the Neural Network model, loss function, and optimizer.\n",
    "    nn_model = ClassifierNN(input_dim_selected, config['hidden_dims_nn'], config['dropout_rates_nn']).to(device)\n",
    "    nn_criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight]).to(device))\n",
    "    nn_optimizer = torch.optim.AdamW(nn_model.parameters(), lr=config['learning_rate'], \n",
    "                                     weight_decay=config['weight_decay'])\n",
    "\n",
    "    # Initialize the learning rate scheduler based on the configuration.\n",
    "    if config['lr_scheduler_type'] == 'cyclic':\n",
    "        nn_scheduler = CyclicLR(nn_optimizer, base_lr=config['min_lr'], max_lr=config['max_lr'],\n",
    "                                step_size_up=2000, cycle_momentum=False)\n",
    "        logging.info(\"Using CyclicLR learning rate scheduler\")\n",
    "    else: # Default to ReduceLROnPlateau\n",
    "        nn_scheduler = ReduceLROnPlateau(nn_optimizer, 'min', patience=config['patience']//2, \n",
    "                                         factor=0.5, verbose=True)\n",
    "        logging.info(\"Using ReduceLROnPlateau learning rate scheduler\")\n",
    "\n",
    "    # Initialize early stopping to prevent overfitting.\n",
    "    nn_early_stopper = EarlyStopping(patience=config['patience'], delta=config['early_stopping_delta'])\n",
    "\n",
    "    # Train the neural network model.\n",
    "    best_nn_model = train_nn_model(\n",
    "        nn_model, train_nn_loader, val_nn_loader, nn_criterion,\n",
    "        nn_optimizer, nn_scheduler, nn_early_stopper,\n",
    "        config['epochs'], device, config['gradient_clip_norm']\n",
    "    )\n",
    "\n",
    "    # Evaluate the trained neural network on the test set.\n",
    "    best_nn_model.eval() # Set the model to evaluation mode for inference\n",
    "    nn_test_probs_list = []\n",
    "    with torch.no_grad(): # Disable gradient calculation for efficient inference\n",
    "        for xb, _ in test_nn_loader:\n",
    "            xb = xb.to(device)\n",
    "            outputs = best_nn_model(xb).squeeze()\n",
    "            nn_test_probs_list.extend(torch.sigmoid(outputs).cpu().numpy()) # Convert logits to probabilities\n",
    "    nn_test_probs = np.array(nn_test_probs_list)\n",
    "    nn_test_preds = (nn_test_probs > 0.5).astype(int) # Convert probabilities to binary predictions\n",
    "\n",
    "    nn_acc, nn_f1, nn_prec, nn_rec, nn_roc_auc, nn_cm = evaluate_classifier_metrics(\n",
    "        y_test, nn_test_preds, nn_test_probs, \"Neural Network (Test)\"\n",
    "    )\n",
    "    # Plot ROC curve for NN\n",
    "    plot_roc_curve(y_test, nn_test_probs, \"Neural Network\", './images/roc_curve_nn.png')\n",
    "\n",
    "    # --- Visualization: Confusion Matrices ---\n",
    "    # Plot confusion matrix for the trained NN model\n",
    "    fig, axes = plt.subplots(1, 1, figsize=(9, 8)) # Create a single subplot for NN\n",
    "    \n",
    "    sns.heatmap(nn_cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "                xticklabels=['Predicted 0', 'Predicted 1'], \n",
    "                yticklabels=['True 0', 'True 1'], ax=axes)\n",
    "    axes.set_xlabel(\"Predicted Label\")\n",
    "    axes.set_ylabel(\"True Label\")\n",
    "    axes.set_title(f\"Neural Network\\nAccuracy: {nn_acc:.4f}, F1: {nn_f1:.4f}\")\n",
    "    \n",
    "    plt.tight_layout() # Adjust subplot parameters for a tight layout\n",
    "    plot_save_path = './images/confusion_matrix_nn.png'\n",
    "    plt.savefig(plot_save_path) # Save the generated plot to a file\n",
    "    logging.info(f\"Confusion matrix for Neural Network saved to '{plot_save_path}'\")\n",
    "    plt.show() # Display the plot\n",
    "\n",
    "    # --- Model Saving ---\n",
    "    # Save the trained NN model, the scaler, and the feature selector for future use\n",
    "    models_dir = './nn_models' # Dedicated directory for NN models\n",
    "    os.makedirs(models_dir, exist_ok=True) # Create the directory if it doesn't already exist\n",
    "    \n",
    "    torch.save(best_nn_model.state_dict(), os.path.join(models_dir, 'best_nn_model.pt'))\n",
    "    joblib.dump(scaler_global, os.path.join(models_dir, 'scaler.pkl')) # Save the data scaler\n",
    "    joblib.dump(feature_selector, os.path.join(models_dir, 'feature_selector.pkl')) # Save the feature selector\n",
    "    \n",
    "    logging.info(f\"Neural Network model and artifacts saved to '{models_dir}'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
