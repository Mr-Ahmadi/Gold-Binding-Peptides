{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51ad5ca4",
   "metadata": {},
   "source": [
    "# PepBERT - Small - UniRef100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "becffd11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved embeddings and t-SNE coordinates to ../data/embeddings/external/Small_UniRef100_PepBERT.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from tokenizers import Tokenizer\n",
    "from huggingface_hub import hf_hub_download\n",
    "import importlib.util\n",
    "\n",
    "# Define the repository ID for the PepBERT model\n",
    "repo_id = \"dzjxzyd/PepBERT-small-UniRef100\"\n",
    "\n",
    "# Function to load a module from the Hugging Face Hub\n",
    "def load_module_from_hub(repo_id: str, filename: str):\n",
    "    file_path = hf_hub_download(repo_id=repo_id, filename=filename)\n",
    "    module_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    spec = importlib.util.spec_from_file_location(module_name, file_path)\n",
    "    module = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(module)\n",
    "    return module\n",
    "\n",
    "# Load model architecture and configuration\n",
    "model_module = load_module_from_hub(repo_id, \"model.py\")\n",
    "config_module = load_module_from_hub(repo_id, \"config.py\")\n",
    "build_transformer = model_module.build_transformer\n",
    "get_config = config_module.get_config\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer_path = hf_hub_download(repo_id=repo_id, filename=\"tokenizer.json\")\n",
    "tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "\n",
    "# Download model weights\n",
    "weights_path = hf_hub_download(repo_id=repo_id, filename=\"tmodel_12.pt\")\n",
    "\n",
    "# Initialize the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "config = get_config()\n",
    "model = build_transformer(\n",
    "    src_vocab_size=tokenizer.get_vocab_size(),\n",
    "    src_seq_len=config[\"seq_len\"],\n",
    "    d_model=config[\"d_model\"]\n",
    ")\n",
    "state = torch.load(weights_path, map_location=device)\n",
    "model.load_state_dict(state[\"model_state_dict\"])\n",
    "model.to(device).eval()\n",
    "\n",
    "# Function to encode a single peptide sequence\n",
    "def encode_peptide(sequence):\n",
    "    # Add start and end tokens\n",
    "    encoded_ids = [tokenizer.token_to_id(\"[SOS]\")] + tokenizer.encode(sequence).ids + [tokenizer.token_to_id(\"[EOS]\")]\n",
    "    input_ids = torch.tensor([encoded_ids], dtype=torch.int64).to(device)\n",
    "    encoder_mask = torch.ones((1, 1, 1, input_ids.size(1)), dtype=torch.int64).to(device)\n",
    "    with torch.no_grad():\n",
    "        emb = model.encode(input_ids, encoder_mask)\n",
    "    # remove SOS/EOS positions and average\n",
    "    emb_core = emb[:, 1:-1, :]\n",
    "    emb_avg = emb_core.mean(dim=1)\n",
    "    return emb_avg.squeeze().cpu().numpy()\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(\"../data/external.csv\")  # ensure .csv extension\n",
    "\n",
    "# Encode all sequences\n",
    "embeddings_list = df[\"Sequence\"].apply(encode_peptide).tolist()\n",
    "# Create embeddings DataFrame\n",
    "embeddings_df = pd.DataFrame(embeddings_list)\n",
    "\n",
    "# Combine original data, embeddings, and t-SNE results\n",
    "result_df = pd.concat([df[[\"Sequence\", \"Class\"]], embeddings_df], axis=1)\n",
    "\n",
    "# Save combined DataFrame to a new CSV\n",
    "output_path = \"../data/embeddings/external/Small_UniRef100_PepBERT.csv\"\n",
    "result_df.to_csv(output_path, index=False)\n",
    "print(f\"Saved embeddings and t-SNE coordinates to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c450d95",
   "metadata": {},
   "source": [
    "# PepBERT - Large - UniRef100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20507cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from tokenizers import Tokenizer\n",
    "from huggingface_hub import hf_hub_download\n",
    "import importlib.util\n",
    "\n",
    "# Define the repository ID for the PepBERT model\n",
    "repo_id = \"dzjxzyd/PepBERT-Large-UniRef100\"\n",
    "\n",
    "# Function to load a module from the Hugging Face Hub\n",
    "def load_module_from_hub(repo_id: str, filename: str):\n",
    "    file_path = hf_hub_download(repo_id=repo_id, filename=filename)\n",
    "    module_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    spec = importlib.util.spec_from_file_location(module_name, file_path)\n",
    "    module = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(module)\n",
    "    return module\n",
    "\n",
    "# Load model architecture and configuration\n",
    "model_module = load_module_from_hub(repo_id, \"model.py\")\n",
    "config_module = load_module_from_hub(repo_id, \"config.py\")\n",
    "build_transformer = model_module.build_transformer\n",
    "get_config = config_module.get_config\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer_path = hf_hub_download(repo_id=repo_id, filename=\"tokenizer.json\")\n",
    "tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "\n",
    "# Download model weights\n",
    "weights_path = hf_hub_download(repo_id=repo_id, filename=\"tmodel_17.pt\")\n",
    "\n",
    "# Initialize the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "config = get_config()\n",
    "model = build_transformer(\n",
    "    src_vocab_size=tokenizer.get_vocab_size(),\n",
    "    src_seq_len=config[\"seq_len\"],\n",
    "    d_model=config[\"d_model\"]\n",
    ")\n",
    "state = torch.load(weights_path, map_location=device)\n",
    "model.load_state_dict(state[\"model_state_dict\"])\n",
    "model.to(device).eval()\n",
    "\n",
    "# Function to encode a single peptide sequence\n",
    "def encode_peptide(sequence):\n",
    "    # Add start and end tokens\n",
    "    encoded_ids = [tokenizer.token_to_id(\"[SOS]\")] + tokenizer.encode(sequence).ids + [tokenizer.token_to_id(\"[EOS]\")]\n",
    "    input_ids = torch.tensor([encoded_ids], dtype=torch.int64).to(device)\n",
    "    encoder_mask = torch.ones((1, 1, 1, input_ids.size(1)), dtype=torch.int64).to(device)\n",
    "    with torch.no_grad():\n",
    "        emb = model.encode(input_ids, encoder_mask)\n",
    "    # remove SOS/EOS positions and average\n",
    "    emb_core = emb[:, 1:-1, :]\n",
    "    emb_avg = emb_core.mean(dim=1)\n",
    "    return emb_avg.squeeze().cpu().numpy()\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(\"../data/external.csv\")  # ensure .csv extension\n",
    "\n",
    "# Encode all sequences\n",
    "embeddings_list = df[\"Sequence\"].apply(encode_peptide).tolist()\n",
    "# Create embeddings DataFrame\n",
    "embeddings_df = pd.DataFrame(embeddings_list)\n",
    "\n",
    "# Combine original data, embeddings, and t-SNE results\n",
    "result_df = pd.concat([df[[\"Sequence\", \"Class\"]], embeddings_df], axis=1)\n",
    "\n",
    "# Save combined DataFrame to a new CSV\n",
    "output_path = \"../data/embeddings/external/Large_UniRef100_PepBERT.csv\"\n",
    "result_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fe5feb",
   "metadata": {},
   "source": [
    "# iFeature - AAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22813577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptor type: AAC\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import subprocess\n",
    "import tempfile\n",
    "\n",
    "# Function to run iFeature for a specific descriptor\n",
    "def extract_ifeature(sequences, descriptor=\"AAC\", ifeature_path=\"iFeature/iFeature.py\"):\n",
    "    # Create a temporary FASTA file\n",
    "    with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".fasta\", delete=False) as temp_fasta:\n",
    "        for i, seq in enumerate(sequences):\n",
    "            temp_fasta.write(f\">{i}\\n{seq}\\n\")\n",
    "        temp_fasta_path = temp_fasta.name\n",
    "\n",
    "    # Output file for features\n",
    "    output_file = tempfile.NamedTemporaryFile(mode=\"r\", suffix=\".csv\", delete=False).name\n",
    "\n",
    "    # Run iFeature command\n",
    "    cmd = [\n",
    "        \"python\", ifeature_path,\n",
    "        \"--file\", temp_fasta_path,\n",
    "        \"--type\", descriptor,\n",
    "        \"--out\", output_file\n",
    "    ]\n",
    "    subprocess.run(cmd, check=True)\n",
    "\n",
    "    # Read the output CSV\n",
    "    features_df = pd.read_csv(output_file, sep=\"\\t\")\n",
    "    # Drop the first column (sequence IDs)\n",
    "    features_df = features_df.drop(columns=[\"#\"])\n",
    "\n",
    "    # Clean up temporary files\n",
    "    os.unlink(temp_fasta_path)\n",
    "    os.unlink(output_file)\n",
    "\n",
    "    return features_df\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(\"../data/external.csv\")  # Ensure .csv extension\n",
    "\n",
    "# Extract features using iFeature (e.g., AAC descriptor)\n",
    "descriptor = \"AAC\"  # You can change to \"DPC\", \"CTD\", etc.\n",
    "ifeature_path = \"iFeature/iFeature.py\"  # Adjust path to your iFeature installation\n",
    "features_df = extract_ifeature(df[\"Sequence\"], descriptor=descriptor, ifeature_path=ifeature_path)\n",
    "\n",
    "# Combine original data, features, and t-SNE results\n",
    "result_df = pd.concat([df[[\"Sequence\", \"Class\"]], features_df], axis=1)\n",
    "\n",
    "# Save combined DataFrame to a new CSV\n",
    "output_path = f\"../data/embeddings/external/iFeature_{descriptor}.csv\"\n",
    "result_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a775bce",
   "metadata": {},
   "source": [
    "# iFeature - DPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c207075b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptor type: DPC\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import subprocess\n",
    "import tempfile\n",
    "\n",
    "# Function to run iFeature for a specific descriptor\n",
    "def extract_ifeature(sequences, descriptor=\"AAC\", ifeature_path=\"iFeature/iFeature.py\"):\n",
    "    # Create a temporary FASTA file\n",
    "    with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".fasta\", delete=False) as temp_fasta:\n",
    "        for i, seq in enumerate(sequences):\n",
    "            temp_fasta.write(f\">{i}\\n{seq}\\n\")\n",
    "        temp_fasta_path = temp_fasta.name\n",
    "\n",
    "    # Output file for features\n",
    "    output_file = tempfile.NamedTemporaryFile(mode=\"r\", suffix=\".csv\", delete=False).name\n",
    "\n",
    "    # Run iFeature command\n",
    "    cmd = [\n",
    "        \"python\", ifeature_path,\n",
    "        \"--file\", temp_fasta_path,\n",
    "        \"--type\", descriptor,\n",
    "        \"--out\", output_file\n",
    "    ]\n",
    "    subprocess.run(cmd, check=True)\n",
    "\n",
    "    # Read the output CSV\n",
    "    features_df = pd.read_csv(output_file, sep=\"\\t\")\n",
    "    # Drop the first column (sequence IDs)\n",
    "    features_df = features_df.drop(columns=[\"#\"])\n",
    "\n",
    "    # Clean up temporary files\n",
    "    os.unlink(temp_fasta_path)\n",
    "    os.unlink(output_file)\n",
    "\n",
    "    return features_df\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(\"../data/external.csv\")  # Ensure .csv extension\n",
    "\n",
    "# Extract features using iFeature (e.g., AAC descriptor)\n",
    "descriptor = \"DPC\"  # You can change to \"DPC\", \"CTD\", etc.\n",
    "ifeature_path = \"iFeature/iFeature.py\"  # Adjust path to your iFeature installation\n",
    "features_df = extract_ifeature(df[\"Sequence\"], descriptor=descriptor, ifeature_path=ifeature_path)\n",
    "\n",
    "# Combine original data, features, and t-SNE results\n",
    "result_df = pd.concat([df[[\"Sequence\", \"Class\"]], features_df], axis=1)\n",
    "\n",
    "# Save combined DataFrame to a new CSV\n",
    "output_path = f\"../data/embeddings/external/iFeature_{descriptor}.csv\"\n",
    "result_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2970ee",
   "metadata": {},
   "source": [
    "# Kidera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "84cb2138",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# 1. Load data\n",
    "df = pd.read_csv('../data/external.csv')  # expects columns: ID, Sequence, Class\n",
    "\n",
    "# 2. Define Kidera factor matrix manually\n",
    "# Source: Kidera et al. (1985), Amino Acids mapped to 10-factor vectors\n",
    "kidera = {\n",
    "    'A': [-1.56, -1.67, -0.97, -0.27, -0.93, -0.78, -0.20, -0.08, 0.21, -0.48],\n",
    "    'C': [1.02, 0.26, 1.77, -0.26, 0.77, -0.27, -0.32, 0.01, 0.05, 0.29],\n",
    "    'D': [0.36, 1.47, 1.07, 0.42, 0.35, -0.23, 0.08, 0.09, -0.70, 0.43],\n",
    "    'E': [-1.01, 0.69, 0.77, 0.38, 0.11, -0.52, -0.11, -0.05, -0.19, 0.17],\n",
    "    'F': [1.13, 0.05, -0.11, 0.35, 1.34, 0.15, 0.50, -0.37, 0.28, -0.09],\n",
    "    'G': [-2.37, 0.08, 0.67, 1.09, -0.98, 0.03, -0.08, 0.07, 0.15, 0.02],\n",
    "    'H': [0.02, -0.66, 0.68, 0.64, 0.05, 0.21, 0.52, -0.20, -0.09, -0.04],\n",
    "    'I': [1.64, -0.08, -1.33, -0.63, 0.50, 0.06, -0.19, -0.23, 0.01, -0.32],\n",
    "    'K': [-0.61, 1.17, 0.60, 0.57, -0.23, -0.31, -0.34, -0.12, 0.45, -0.19],\n",
    "    'L': [1.56, -0.09, -1.36, -0.58, 0.47, 0.05, -0.21, -0.21, -0.03, -0.26],\n",
    "    'M': [1.12, 0.27, -0.39, -0.30, 0.08, -0.02, 0.21, -0.06, 0.08, -0.14],\n",
    "    'N': [0.05, 0.90, 0.75, 0.73, -0.29, 0.08, 0.04, 0.11, -0.40, 0.27],\n",
    "    'P': [-1.51, -0.04, 0.73, 0.63, -0.43, 0.16, 0.18, 0.07, -0.03, -0.06],\n",
    "    'Q': [-0.10, 0.59, 0.81, 0.50, -0.26, -0.01, 0.08, 0.04, -0.36, 0.19],\n",
    "    'R': [0.80, 1.63, 0.13, 0.14, 0.06, -0.20, -0.33, -0.09, 0.04, -0.15],\n",
    "    'S': [-0.38, 0.48, 1.15, 0.70, -0.49, 0.10, 0.05, 0.07, -0.29, 0.21],\n",
    "    'T': [-0.22, 0.02, 0.88, 0.72, 0.03, 0.09, 0.05, 0.09, -0.01, 0.01],\n",
    "    'V': [1.36, -0.06, -1.04, -0.66, 0.36, 0.03, -0.13, -0.19, -0.01, -0.23],\n",
    "    'W': [0.81, -0.09, -0.73, 0.56, 2.00, 0.15, 0.68, -0.46, 0.47, -0.20],\n",
    "    'Y': [-0.07, -0.30, 0.03, 0.35, 1.07, -0.15, 0.31, -0.37, 0.40, -0.04]\n",
    "}\n",
    "\n",
    "# Function to compute mean Kidera factors per sequence\n",
    "def seq_kidera(seq):\n",
    "    factors = []\n",
    "    for aa in seq:\n",
    "        if aa in kidera:\n",
    "            factors.append(kidera[aa])\n",
    "        else:\n",
    "            sys.exit(f\"Error: Unknown amino acid '{aa}' in sequence.\")\n",
    "    return np.mean(factors, axis=0)\n",
    "\n",
    "# Compute Kidera features for all sequences\n",
    "kf_matrix = np.vstack(df['Sequence'].apply(seq_kidera))\n",
    "kf_df = pd.DataFrame(kf_matrix, columns=[f'KF{i+1}' for i in range(kf_matrix.shape[1])])\n",
    "\n",
    "# Create a combined DataFrame including Sequence, Class, Kidera features, and t-SNE results\n",
    "kidera_encoded_df = pd.concat([df[['Sequence', 'Class']], kf_df], axis=1)\n",
    "\n",
    "# Define the output path for the Kidera encoded CSV\n",
    "output_dir = '../data/embeddings/external'\n",
    "os.makedirs(output_dir, exist_ok=True) # Ensure the directory exists\n",
    "output_path = os.path.join(output_dir, 'Kidera_encoded.csv')\n",
    "\n",
    "# Save the combined DataFrame to CSV\n",
    "kidera_encoded_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa653c89",
   "metadata": {},
   "source": [
    "# BLOSUM62"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6579571b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Load data\n",
    "# Assumes 'train.csv' is in a 'data' directory one level up from the script's location.\n",
    "# The DataFrame is expected to have 'ID', 'Sequence', and 'Class' columns.\n",
    "try:\n",
    "    df = pd.read_csv('../data/external.csv')\n",
    "except FileNotFoundError:\n",
    "    sys.exit(\"Error: 'train.csv' not found. Please ensure it's in '../data/'.\")\n",
    "except Exception as e:\n",
    "    sys.exit(f\"Error loading data: {e}\")\n",
    "\n",
    "# Ensure the 'Sequence' and 'Class' columns exist\n",
    "if 'Sequence' not in df.columns:\n",
    "    sys.exit(\"Error: 'Sequence' column not found in the input DataFrame.\")\n",
    "if 'Class' not in df.columns:\n",
    "    sys.exit(\"Error: 'Class' column not found in the input DataFrame.\")\n",
    "if df['Sequence'].empty:\n",
    "    sys.exit(\"Error: 'Sequence' column is empty in the input DataFrame.\")\n",
    "\n",
    "# 2. Define BLOSUM62 matrix manually\n",
    "# This matrix represents the substitution scores for amino acid pairs.\n",
    "# Source: Derived from standard BLOSUM62 matrices (e.g., NCBI).\n",
    "# The order of amino acids in the dictionary keys and inner dictionary keys is crucial\n",
    "# for consistent vector representation.\n",
    "blosum62 = {\n",
    "    'A': {'A': 4, 'R': -1, 'N': -2, 'D': -2, 'C': 0, 'Q': -1, 'E': -1, 'G': 0, 'H': -2, 'I': -1, 'L': -1, 'K': -1, 'M': -1, 'F': -2, 'P': -1, 'S': 1, 'T': 0, 'W': -3, 'Y': -2, 'V': 0},\n",
    "    'R': {'A': -1, 'R': 5, 'N': 0, 'D': -2, 'C': -3, 'Q': 1, 'E': 0, 'G': -2, 'H': 0, 'I': -3, 'L': -2, 'K': 2, 'M': -1, 'F': -3, 'P': -2, 'S': -1, 'T': -1, 'W': -3, 'Y': -2, 'V': -3},\n",
    "    'N': {'A': -2, 'R': 0, 'N': 6, 'D': 1, 'C': -3, 'Q': 0, 'E': 0, 'G': 0, 'H': 1, 'I': -3, 'L': -3, 'K': 0, 'M': -2, 'F': -3, 'P': -2, 'S': 1, 'T': 0, 'W': -4, 'Y': -2, 'V': -3},\n",
    "    'D': {'A': -2, 'R': -2, 'N': 1, 'D': 6, 'C': -3, 'Q': 0, 'E': 2, 'G': -1, 'H': -1, 'I': -3, 'L': -4, 'K': -1, 'M': -3, 'F': -3, 'P': -1, 'S': 0, 'T': -1, 'W': -4, 'Y': -3, 'V': -3},\n",
    "    'C': {'A': 0, 'R': -3, 'N': -3, 'D': -3, 'C': 9, 'Q': -3, 'E': -4, 'G': -3, 'H': -3, 'I': -1, 'L': -1, 'K': -3, 'M': -1, 'F': -2, 'P': -3, 'S': -1, 'T': -1, 'W': -2, 'Y': -2, 'V': -1},\n",
    "    'Q': {'A': -1, 'R': 1, 'N': 0, 'D': 0, 'C': -3, 'Q': 5, 'E': 2, 'G': -2, 'H': 0, 'I': -3, 'L': -2, 'K': 1, 'M': 0, 'F': -3, 'P': -1, 'S': 0, 'T': -1, 'W': -2, 'Y': -1, 'V': -2},\n",
    "    'E': {'A': -1, 'R': 0, 'N': 0, 'D': 2, 'C': -4, 'Q': 2, 'E': 5, 'G': -2, 'H': 0, 'I': -3, 'L': -3, 'K': 1, 'M': -2, 'F': -3, 'P': -1, 'S': 0, 'T': -1, 'W': -3, 'Y': -2, 'V': -2},\n",
    "    'G': {'A': 0, 'R': -2, 'N': 0, 'D': -1, 'C': -3, 'Q': -2, 'E': -2, 'G': 6, 'H': -2, 'I': -4, 'L': -4, 'K': -2, 'M': -3, 'F': -3, 'P': -2, 'S': 0, 'T': -2, 'W': -2, 'Y': -3, 'V': -3},\n",
    "    'H': {'A': -2, 'R': 0, 'N': 1, 'D': -1, 'C': -3, 'Q': 0, 'E': 0, 'G': -2, 'H': 8, 'I': -3, 'L': -3, 'K': -1, 'M': -2, 'F': -1, 'P': -2, 'S': -1, 'T': -2, 'W': -2, 'Y': 2, 'V': -3},\n",
    "    'I': {'A': -1, 'R': -3, 'N': -3, 'D': -3, 'C': -1, 'Q': -3, 'E': -3, 'G': -4, 'H': -3, 'I': 4, 'L': 2, 'K': -3, 'M': 1, 'F': 0, 'P': -3, 'S': -2, 'T': -1, 'W': -3, 'Y': -1, 'V': 3},\n",
    "    'L': {'A': -1, 'R': -2, 'N': -3, 'D': -4, 'C': -1, 'Q': -2, 'E': -3, 'G': -4, 'H': -3, 'I': 2, 'L': 4, 'K': -2, 'M': 2, 'F': 0, 'P': -3, 'S': -2, 'T': -1, 'W': -2, 'Y': -1, 'V': 1},\n",
    "    'K': {'A': -1, 'R': 2, 'N': 0, 'D': -1, 'C': -3, 'Q': 1, 'E': 1, 'G': -2, 'H': -1, 'I': -3, 'L': -2, 'K': 5, 'M': -1, 'F': -3, 'P': -1, 'S': 0, 'T': -1, 'W': -3, 'Y': -2, 'V': -2},\n",
    "    'M': {'A': -1, 'R': -1, 'N': -2, 'D': -3, 'C': -1, 'Q': 0, 'E': -2, 'G': -3, 'H': -2, 'I': 1, 'L': 2, 'K': -1, 'M': 5, 'F': 0, 'P': -2, 'S': -1, 'T': -1, 'W': -2, 'Y': -1, 'V': 1},\n",
    "    'F': {'A': -2, 'R': -3, 'N': -3, 'D': -3, 'C': -2, 'Q': -3, 'E': -3, 'G': -3, 'H': -1, 'I': 0, 'L': 0, 'K': -3, 'M': 0, 'F': 6, 'P': -4, 'S': -2, 'T': -2, 'W': 1, 'Y': 3, 'V': -1},\n",
    "    'P': {'A': -1, 'R': -2, 'N': -2, 'D': -1, 'C': -3, 'Q': -1, 'E': -1, 'G': -2, 'H': -2, 'I': -3, 'L': -3, 'K': -1, 'M': -2, 'F': -4, 'P': 7, 'S': -1, 'T': -1, 'W': -4, 'Y': -3, 'V': -2},\n",
    "    'S': {'A': 1, 'R': -1, 'N': 1, 'D': 0, 'C': -1, 'Q': 0, 'E': 0, 'G': 0, 'H': -1, 'I': -2, 'L': -2, 'K': 0, 'M': -1, 'F': -2, 'P': -1, 'S': 4, 'T': 1, 'W': -3, 'Y': -2, 'V': -2},\n",
    "    'T': {'A': 0, 'R': -1, 'N': 0, 'D': -1, 'C': -1, 'Q': -1, 'E': -1, 'G': -2, 'H': -2, 'I': -1, 'L': -1, 'K': -1, 'M': -1, 'F': -2, 'P': -1, 'S': 1, 'T': 5, 'W': -2, 'Y': -2, 'V': 0},\n",
    "    'W': {'A': -3, 'R': -3, 'N': -4, 'D': -4, 'C': -2, 'Q': -2, 'E': -3, 'G': -2, 'H': -2, 'I': -3, 'L': -2, 'K': -3, 'M': -2, 'F': 1, 'P': -4, 'S': -3, 'T': -2, 'W': 11, 'Y': 2, 'V': -3},\n",
    "    'Y': {'A': -2, 'R': -2, 'N': -2, 'D': -3, 'C': -2, 'Q': -1, 'E': -2, 'G': -3, 'H': 2, 'I': -1, 'L': -1, 'K': -2, 'M': -1, 'F': 3, 'P': -3, 'S': -2, 'T': -2, 'W': 2, 'Y': 7, 'V': -1},\n",
    "    'V': {'A': 0, 'R': -3, 'N': -3, 'D': -3, 'C': -1, 'Q': -2, 'E': -2, 'G': -3, 'H': -3, 'I': 3, 'L': 1, 'K': -2, 'M': 1, 'F': -1, 'P': -2, 'S': -2, 'T': 0, 'W': -3, 'Y': -1, 'V': 4}\n",
    "}\n",
    "\n",
    "# Get an ordered list of amino acids to ensure consistent vector dimensions and order.\n",
    "amino_acids = sorted(list(blosum62.keys()))\n",
    "\n",
    "# Function to compute mean BLOSUM62 features per sequence\n",
    "def seq_blosum62(seq):\n",
    "    \"\"\"\n",
    "    Computes a fixed-size feature vector for a given peptide sequence\n",
    "    by averaging the BLOSUM62 row vectors for each amino acid in the sequence.\n",
    "\n",
    "    Args:\n",
    "        seq (str): The peptide sequence.\n",
    "\n",
    "    Returns:\n",
    "        np.array: A 20-dimensional numpy array representing the mean BLOSUM62 features.\n",
    "    \"\"\"\n",
    "    blosum_vectors = []\n",
    "    for aa in seq:\n",
    "        if aa in blosum62:\n",
    "            # Extract the BLOSUM62 row for the current amino acid,\n",
    "            # ensuring the order of elements matches 'amino_acids'.\n",
    "            aa_vector = [blosum62[aa][other_aa] for other_aa in amino_acids]\n",
    "            blosum_vectors.append(aa_vector)\n",
    "        else:\n",
    "            # If an unknown amino acid is encountered, terminate with an error.\n",
    "            sys.exit(f\"Error: Unknown amino acid '{aa}' in sequence '{seq}'.\")\n",
    "\n",
    "    # Handle empty sequences by returning a zero vector of the correct dimension.\n",
    "    if not blosum_vectors:\n",
    "        return np.zeros(len(amino_acids))\n",
    "    \n",
    "    # Compute the mean of all collected BLOSUM62 row vectors.\n",
    "    return np.mean(blosum_vectors, axis=0)\n",
    "\n",
    "# Compute BLOSUM62 features for all sequences in the DataFrame.\n",
    "# This applies the 'seq_blosum62' function to each sequence and stacks the results.\n",
    "blosum_matrix = np.vstack(df['Sequence'].apply(seq_blosum62))\n",
    "# Create a DataFrame from the computed features with descriptive column names.\n",
    "blosum_df = pd.DataFrame(blosum_matrix, columns=[f'BLOSUM62_{aa}' for aa in amino_acids])\n",
    "\n",
    "# Create a combined DataFrame including Sequence, Class, BLOSUM62 features, and t-SNE results.\n",
    "blosum_encoded_df = pd.concat([df[['Sequence', 'Class']], blosum_df], axis=1)\n",
    "\n",
    "# Define the output directory and file path for the encoded data.\n",
    "output_dir = '../data/embeddings/'\n",
    "# Create the output directory if it doesn't already exist.\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_path = os.path.join(output_dir, 'external/BLOSUM62_encoded.csv')\n",
    "\n",
    "# Save the combined DataFrame to a CSV file without the DataFrame index.\n",
    "blosum_encoded_df.to_csv(output_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
